global:
  runningOnAws: false
  cluster:
    name: "my-cluster"
    domain: "example.com"
    domain_id: ""
    egressIpAddresses: ["127.0.0.1"]
  account:
    name: ""
  roles:
    canary: ""
  cloudHsm:
    enabled: false
    ip: "127.0.0.1"
  kubeApiService:
    endpointCidrs: []
  # move these to gsp-namespace terraform output
  canary:
    repository: ""
    verificationKeys: []
  ci:
    privateKey: ""
    publicKey: ""

adminRoleARNs: []
adminUserARNs: []
sreRoleARNs: []
sreUserARNs: []
permittedRolesRegex: ""
istioSystemRoleRegex: ""

githubAPIToken: ""

googleOauthClientId: ""
googleOauthClientSecret: ""

httpsEgressSafelist: []
httpEgressSafelist: []


# users:
# - name: chris.farmiloe
#   roleARN: xxx/user.chris.farmiloe
#   groups:
#   - sandbox-admin
#   - sandbox-sre
#   - sandbox-canary-dev
# - name: sam.crang
#   roleARN: xxx/user.sam.crang
#   groups:
#   - sandbox-canary-dev

# namespaces:
# - name: verify-metadata-controller
#   owner: alphagov
#   repository: verify-metadata-controller
#   branch: master
#   path: ci
#   permittedRolesRegex: "^$"
#   requiredApprovalCount: 2
#   scope: cluster

cluster-autoscaler:
  extraArgs:
    balance-similar-node-groups: true
  image:
    tag: v1.14.5 # upgrade this when upgrading kubernetes
  rbac:
    create: true
  # we can only set this if cluster-autoscaler is in the kube-system namespace D:
  # priorityClassName: system-cluster-critical
  serviceMonitor:
    enabled: true
    namespace: gsp-system
  nodeSelector:
    node-role.kubernetes.io/cluster-management: ""
  tolerations:
  - effect: NoSchedule
    key: node-role.kubernetes.io/cluster-management

kiam:
  nameOverride:
  fullnameOverride:
  server:
    tlsFiles:
      ca: "NADA"
      cert: "NADA"
      key: "NADA"
    nodeSelector:
      node-role.kubernetes.io/cluster-management: ""
    tolerations:
      - effect: NoSchedule
        key: node-role.kubernetes.io/cluster-management
    log:
      level: info
    probes:
      serverAddress: 127.0.0.1
    extraHostPathMounts:
      - name: ssl-certs
        mountPath: /etc/ssl/certs/ca-certificates.crt
        hostPath: /etc/pki/tls/certs/ca-bundle.crt
        readOnly: true
    serviceAnnotations:
      networking.istio.io/exportTo: "."
  agent:
    gatewayTimeoutCreation: 30s
    host:
      iptables: true
    tlsFiles:
      ca: "NADA"
      cert: "NADA"
      key: "NADA"
    log:
      level: info
    whiteListRouteRegexp: "^(/latest/meta-data/placement/availability-zone)$"
    tolerations:
    - key: node-role.kubernetes.io/ci
      effect: NoSchedule
      operator: Exists
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoExecute
      operator: Exists

fluentd-cloudwatch:
  resources:
    limits:
      memory: 512Mi
    requests:
      memory: 512Mi
  image:
    tag: v1.3.2-debian-cloudwatch  # More recent image needed for fluentd-kubernetes-daemonset to avoid utf-8 encoding errors
  rbac:
    create: true
  awsRegion: eu-west-2
  extraVars:
    - "{ name: FLUENT_UID, value: '0' }"  # run fluentd as root as instructed by https://github.com/helm/charts/tree/master/incubator/fluentd-cloudwatch
  updateStrategy:
    type: RollingUpdate
  tolerations:
    - key: node-role.kubernetes.io/master
      effect: NoSchedule

concourse:
  web:
    nameOverride: concourse-web
    additionalVolumes:
    - name: ci-web-configuration
      configMap:
        name: gsp-concourse
    additionalVolumeMounts:
    - name: ci-web-configuration
      mountPath: /web-configuration
    resources:
      requests:
        cpu: 100m
        memory: 1Gi
  monitor:
    create: true
  worker:
    nameOverride: concourse-worker
    replicas: 2
    hardAntiAffinity: true
    resources:
      requests:
        cpu: 150m
        memory: 2Gi
    env:
    - name: CONCOURSE_GARDEN_DNS_PROXY_ENABLE
      value: "false"
    - name: CONCOURSE_WORKER_GARDEN_DNS_PROXY_ENABLE
      value: "false"
  secrets:
    localUsers: admin:password
  concourse:
    worker:
      logLevel: error
      ephemeral: true
      baggageclaim:
        logLevel: error
        driver: overlay
    web:
      xFrameOptions: "allow-from: https://framesplits.cloudapps.digital/"
      logLevel: error
      auth:
        mainTeam:
          localUser: admin
      kubernetes:
        createTeamNamespaces: false
      service:
        type: ClusterIP
      prometheus:
        enabled: true
      enablePipelineAuditing: true
      enableTeamAuditing: true
  persistence:
    worker:
      size: 128Gi
  postgresql:
    persistence:
      size: 64Gi

pipelineOperator:
  service:
    port: 443
  serviceAccountName: pipeline-operator-service-account
  image:
    repository: govsvc/concourse-operator
    tag: latest
  concourseUsername: admin
  concoursePassword: password
  concourseInsecureSkipVerify: "false"

serviceOperator:
  service:
    port: 443
  serviceAccountName: service-operator-service-account
  image:
    repository: govsvc/service-operator
    tag: latest

harbor:
  harborAdminPassword: ""
  chartmuseum:
    enabled: false
  logLevel: warn
  expose:
    type: ingress
    tls:
      enabled: false
  core:
    replicas: 2
  registry:
    replicas: 2
  notary:
    server:
      replicas: 4
    signer:
      replicas: 4
    tlsCA: ""
    tlsCert: ""
    tlsKey: ""

# sealed-secrets keys
secrets:
  public_certificate: ""
  private_key: ""

letsencrypt:
  email: daniel.blair@digital.cabinet-office.gov.uk

# concourse resource image references available in-cluster
# this lets users follow our upstream builds.
# these values will be converted into a Secret available
# in user namespaces so they can be referenced in concourse
# as ((concourse.github-resource-image)) etc.
concourseResources:
  github:
    image:
      repository: govsvc/concourse-github-resource
      tag: latest
  harbor:
    image:
      repository: govsvc/concourse-harbor-resource
      tag: latest
  task:
    image:
      repository: govsvc/task-toolbox
      tag: latest

externalDns:
  iamRoleName: ""
