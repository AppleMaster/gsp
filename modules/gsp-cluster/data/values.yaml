global:
  runningOnAws: true
  cluster:
    name: ${cluster_name}
    domain: ${cluster_domain}
    domain_id: ${cluster_domain_id}
  account:
    name: ${account_name}
  roles:
    harbor: ${harbor_iam_role_name}
    concourse: ${concourse_iam_role_name}
  cloudHsm:
    enabled: false
    ip: "127.0.0.1"
  # move these to gsp-namespace terraform output
  canary:
    verificationKeys: []
    
adminRoleARNs: ${admin_role_arns}
adminUserARNs: ${admin_user_arns}
sreRoleARNs: ${sre_role_arns}
sreUserARNs: ${sre_user_arns}
devRoleARNs: []
bootstrapRoleARNs: ${bootstrap_role_arns}

permittedRolesRegex: ${permitted_roles_regex}

httpsEgressSafelist: []

notary:
  rootPassphrase: ${notary_root_passphrase}
  rootKey: ${notary_root_key}
  targetsPassphrase: ${notary_targets_passphrase}
  snapshotPassphrase: ${notary_snapshot_passphrase}
  delegationPassphrase: ${notary_delegation_passphrase}
  delegationKey: ${notary_delegation_key}

concourseMainTeamGithubTeams: ${concourse_main_team_github_teams}
concourse:
  secrets:
    localUsers: >-
      pipeline-operator:${concourse_admin_password}
    githubClientId: ${github_client_id}
    githubClientSecret: ${github_client_secret}
    githubCaCert: ${github_ca_cert}
  worker:
    replicas: ${concourse_worker_count}
    nodeSelector:
      node-role.kubernetes.io/ci: ""
    annotations:
      iam.amazonaws.com/role: ${concourse_iam_role_name}
    tolerations:
      - key: "node-role.kubernetes.io/ci"
        operator: Exists
        effect: NoSchedule
  concourse:
    web:
      externalUrl: https://ci.${cluster_domain}
      auth:
        github:
          enabled: true
        mainTeam:
          localUser: pipeline-operator
          config: /web-configuration/config.yaml
      kubernetes:
        namespacePrefix: ${account_name}-
        createTeamNamespaces: false
        teams: ${concourse_teams}

pipelineOperator:
  concourseUsername: pipeline-operator
  concoursePassword: >-
    ${concourse_admin_password}

harbor:
  harborAdminPassword: ${harbor_admin_password}
  secretKey: ${harbor_secret_key}
  externalURL: https://registry.${cluster_domain}
  expose:
    ingress:
      hosts:
        core: registry.${cluster_domain}
        notary: notary.${cluster_domain}
  persistence:
    imageChartStorage:
      type: s3
      s3:
        bucket: ${harbor_bucket_id}
        region: ${harbor_bucket_region}
        regionendpoint: s3.${harbor_bucket_region}.amazonaws.com
    persistentVolumeClaim:
      database:
        size: 2Gi
  core:
    secret: ${harbor_secret_key}
  jobservice:
    secret: ${harbor_secret_key}
  registry:
    secret: ${harbor_secret_key}
    podAnnotations:
      iam.amazonaws.com/role: ${harbor_iam_role_name}
  chartmuseum:
    podAnnotations:
      iam.amazonaws.com/role: ${harbor_iam_role_name}
  notary:
    secretName: gsp-harbor-notary-tls
    tlsCA: ${notary_ca_pem}
    tlsCert: ${notary_cert_pem}
    tlsKey: ${notary_root_key}

secrets:
  public_certificate: ${sealed_secrets_public_cert}
  private_key: ${sealed_secrets_private_key}

kiam:
  server:
    assumeRoleArn: ${kiam_server_role_arn}
    roleBaseArn: "arn:aws:iam::${account_id}:role/"
    # Hack to trigger a restart of Kiam server on each and every deploy. This
    # is to workaround the agent and server using different sets of
    # CAs/keys/certs because we are currently regenerating CAs/keys/certs on
    # each and every deploy.
    extraEnv:
      restartAfterDeployHack: ${kiam_restart_after_deploy_hack_uuid}
    updateStrategy: RollingUpdate
  agent:
    host:
      interface: "eni+"
    # Hack to trigger a restart of Kiam agent on each and every deploy. This is
    # to workaround the agent and server using different sets of CAs/keys/certs
    # because we are currently regenerating CAs/keys/certs on each and every
    # deploy.
    extraEnv:
      restartAfterDeployHack: ${kiam_restart_after_deploy_hack_uuid}
    updateStrategy: RollingUpdate

fluentd-cloudwatch:
  logGroupName: ${cloudwatch_log_group_name}
  awsRole: ${cloudwatch_log_shipping_role}
  tolerations:
  - operator: Exists
    effect: NoSchedule

prometheus-operator:
  prometheus:
    prometheusSpec:
      externalLabels:
        clustername: ${cluster_domain}
        product: ${account_name}
      additionalAlertManagerConfigs:
      - static_configs:
        - targets:
          - "alerts-1.monitoring.gds-reliability.engineering"
          - "alerts-2.monitoring.gds-reliability.engineering"
          - "alerts-3.monitoring.gds-reliability.engineering"
        scheme: https

